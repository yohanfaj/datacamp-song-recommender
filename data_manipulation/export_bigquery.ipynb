{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importer le client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Spécifiez le chemin vers votre fichier JSON\n",
    "client = bigquery.Client.from_service_account_json('primal-pod-401712-dcbbeb5f006a.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Créer un ensemble de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset primal-pod-401712.datacampSongs\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "dataset_id = \"{}.datacampSongs\".format(client.project)\n",
    "\n",
    "# Construct a full Dataset object to send to the API.\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "\n",
    "dataset.location = \"EU\"\n",
    "\n",
    "# Send the dataset to the API for creation, with an explicit timeout.\n",
    "# Raises google.api_core.exceptions.Conflict if the Dataset already\n",
    "# exists within the project.\n",
    "dataset = client.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Créer la table Songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table primal-pod-401712.Songs\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Spécifiez le chemin vers votre fichier JSON\n",
    "client = bigquery.Client.from_service_account_json('primal-pod-401712-dcbbeb5f006a.json') \n",
    "\n",
    "# ID du dataset et de la table\n",
    "dataset_id = \"{}.datacampSongs\".format(client.project)\n",
    "table_id = \"{}.Songs\".format(dataset_id)\n",
    "\n",
    "# Définition du schéma de la table\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"id\", \"INT64\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"Name\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"Artist\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"Album\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"Cluster\", \"INT64\", mode=\"NULLABLE\")\n",
    "]\n",
    "\n",
    "# Création de la table dans BigQuery\n",
    "table = bigquery.Table(table_id, schema=schema)\n",
    "table = client.create_table(table)  # API request\n",
    "print(\"Created table {}.{}\".format(table.project, table.table_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got table 'primal-pod-401712.datacampSongs.Songs'.\n",
      "Table schema: [SchemaField('id', 'INTEGER', 'REQUIRED', None, None, (), None), SchemaField('Name', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('Artist', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('Album', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('Cluster', 'INTEGER', 'NULLABLE', None, None, (), None)]\n",
      "Table description: None\n",
      "Table has 0 rows\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "\n",
    "# ID du dataset et de la table\n",
    "dataset_id = \"{}.datacampSongs\".format(client.project)\n",
    "table_id = \"{}.Songs\".format(dataset_id)\n",
    "\n",
    "table = client.get_table(table_id)  # Make an API request.\n",
    "\n",
    "# View table properties\n",
    "print(\n",
    "    \"Got table '{}.{}.{}'.\".format(table.project, table.dataset_id, table.table_id)\n",
    ")\n",
    "print(\"Table schema: {}\".format(table.schema))\n",
    "print(\"Table description: {}\".format(table.description))\n",
    "print(\"Table has {} rows\".format(table.num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Album</th>\n",
       "      <th>Cluster_Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A Whiter Shade Of Pale</td>\n",
       "      <td>Procol Harum</td>\n",
       "      <td>A Whiter Shade Of Pale</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>My Sweet Lord</td>\n",
       "      <td>George Harrison</td>\n",
       "      <td>All Things Must Pass (Remastered)</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Bridge Over Troubled Water</td>\n",
       "      <td>Simon &amp; Garfunkel</td>\n",
       "      <td>Bridge Over Troubled Water</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>All Along the Watchtower</td>\n",
       "      <td>Jimi Hendrix</td>\n",
       "      <td>Electric Ladyland</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>In the Ghetto</td>\n",
       "      <td>Elvis Presley</td>\n",
       "      <td>Elvis 30 #1 Hits</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                        Name             Artist   \n",
       "0   0      A Whiter Shade Of Pale       Procol Harum  \\\n",
       "1   1               My Sweet Lord    George Harrison   \n",
       "2   2  Bridge Over Troubled Water  Simon & Garfunkel   \n",
       "3   3    All Along the Watchtower       Jimi Hendrix   \n",
       "4   4               In the Ghetto      Elvis Presley   \n",
       "\n",
       "                               Album  Cluster_Labels  \n",
       "0             A Whiter Shade Of Pale              17  \n",
       "1  All Things Must Pass (Remastered)              17  \n",
       "2         Bridge Over Troubled Water              19  \n",
       "3                  Electric Ladyland               4  \n",
       "4                   Elvis 30 #1 Hits               9  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset_cluster.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 471 rows into primal-pod-401712.datacampSongs.Songs.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Spécifiez le chemin vers votre fichier JSON\n",
    "client = bigquery.Client.from_service_account_json('primal-pod-401712-dcbbeb5f006a.json') \n",
    "\n",
    "# ID du dataset et de la table\n",
    "dataset_id = \"{}.datacampSongs\".format(client.project)\n",
    "table_id = \"{}.Songs\".format(dataset_id)\n",
    "\n",
    "# Spécifiez le chemin vers votre fichier CSV\n",
    "file_path = \"dataset_cluster.csv\"\n",
    "\n",
    "# Configuration de l'importation\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    source_format=bigquery.SourceFormat.CSV,\n",
    "    skip_leading_rows=1,  # Pour sauter l'en-tête du fichier CSV\n",
    "    autodetect=True,  # Pour détecter automatiquement le type de données et le schéma\n",
    "    schema=[\n",
    "        bigquery.SchemaField(\"id\", \"INT64\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"Name\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"Artist\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"Album\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"Cluster\", \"INT64\", mode=\"NULLABLE\")  # Mapping Cluster_Labels du CSV à Cluster dans la DB\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Charger le fichier CSV dans BigQuery\n",
    "with open(file_path, \"rb\") as csv_file:\n",
    "    load_job = client.load_table_from_file(\n",
    "        csv_file, table_id, job_config=job_config\n",
    "    )\n",
    "    load_job.result()  # Attendez que le job d'importation soit terminé\n",
    "\n",
    "print(f\"Loaded {load_job.output_rows} rows into {table_id}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
